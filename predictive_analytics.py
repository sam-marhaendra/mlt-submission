# -*- coding: utf-8 -*-
"""predictive_analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aAoPRbL_YoPCcJQV74ot_FkT6WgY50yg

# **Laptop Prices Prediction**

**Dataset source: https://www.kaggle.com/datasets/anubhavgoyal10/laptop-prices-dataset**

# **Preparation**
"""

!pip install kaggle

!mkdir -p ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d anubhavgoyal10/laptop-prices-dataset

!unzip /content/laptop-prices-dataset.zip

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import mean_absolute_error
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('/content/laptopPrice.csv')

"""# **Exploratory Data Analysis**

## **1. Variable Descriptions**
"""

df.head(3)

df.info()

df.describe()

for col in df.columns:
  if col == 'Price' or col == 'Number of Ratings' or col == 'Number of Reviews':
    continue
  else:
    print(col)
    print(df[col].unique())

"""## **2. Missing Value & Outlier Handling**"""

df.isna().sum()

"""From this above cell output, we can see that there is no missing value on this dataset."""

sns.boxplot(x=df['Number of Ratings'])

sns.boxplot(x=df['Number of Reviews'])

sns.boxplot(x=df['Price'])

"""From the above plots, we can see that all of the numerical features have outliers. So, we need to remove them first."""

# Outlier handling using IQR method
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
df = df[~((df<(Q1-1.5*IQR))|(df>(Q3+1.5*IQR))).any(axis=1)]

print(f'Shape of the data after outlier removal: {df.shape}')

"""## **3. Univariate Analysis**"""

sns.set(style="whitegrid")  # Set the style of the plots

cat_features = df.select_dtypes(include='object').columns.to_list()

num_plots = len(cat_features)  # Get the number of cat_features
num_rows = 8
num_cols = 2

fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 25))  # Create subplots

for i, column in enumerate(cat_features):
    row = i // num_cols  # Calculate the row index
    col = i % num_cols  # Calculate the column index

    # Create a count plot for each column
    ax = axes[row, col] if num_plots > 1 else axes  # Use a single axis if there is only one plot
    sns.countplot(x=column, data=df, palette="viridis", ax=ax)  # Count plot for the current column

# Remove any empty subplots
if num_plots < (num_rows * num_cols):
    for i in range(num_plots, num_rows * num_cols):
        fig.delaxes(axes.flatten()[i])

# Adjust the layout and spacing of the subplots
plt.tight_layout()

# Show the plot
plt.show()

"""**Several insights from the above plots:**

- The top 4 common brands are ASUS, DELL, Lenovo, and HP
- There are three different kinds of the processor brand, there are Intel, AMD, and M1
- The most common processor name is Core i5
- Most laptops aren't supported by the graphics card
- The most common OS is Windows 64-bit
- The laptop rating values centered on 3 and 4 stars.
"""

fig, axes = plt.subplots(1, 3, figsize=(15, 3))

sns.histplot(df['Number of Ratings'], kde=True, bins=10, ax=axes[0])
sns.histplot(df['Number of Reviews'], kde=True, bins=10, ax=axes[1])
sns.histplot(df['Price'], kde=True, bins=10, ax=axes[2])

plt.show()

"""**Several insights from the above plots:**

- The distribution of the `Number of Ratings` and `Number of Reviews` columns are right-skewed
- About half of the laptop price lies on below $80000.

## **4. Multivariate Analysis**
"""

sns.pairplot(df, diag_kind='kde', height=2, aspect=3)

"""**Insight from the above plots:**

- `Number of Ratings` and `Number of Reviews` features seems to have high correlation each other, but have low correlation with the `Price` column.
"""

cat_features = df.select_dtypes(include='object').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y="Price", kind="bar", dodge=False, height=4, aspect=4, data=df)
  plt.title("Average `Price` relative to `{}`".format(col))

plt.show()

"""**Insights from the above plots:**

- From the `processor_name` feature, we can see that as the price increase, the higher the processor is
- There is increase in laptop price as the higher the graphic card specification.
"""

plt.figure(figsize=(5, 3))
correlation_matrix = df.corr().round(2)
 
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Heatmap for Numerical Features")

"""**Insight from the above plot:**

- Even though it was previously stated that the `Number of Reviews` and `Number of Ratings` features don't have a high correlation with the `Price` column, based on the above correlation heatmap, the decision is to keep both features as both of them have absolute correlation value to the target column above 0.1.

# **Data Preparation**

## **1. Encode Categorical Features**

To be able to transform the categorical features to the numerical ones so that they can be fed into the model.
"""

df = pd.get_dummies(data=df, columns=cat_features)
df.head()

"""## **2. PCA**

To reduce the dimensionality of the data while still keeping most of the information on the data.
"""

from sklearn.decomposition import PCA
 
pca = PCA(n_components=2, random_state=123)
pca.fit(df[['Number of Ratings', 'Number of Reviews']])

princ_comp = pca.transform(df[['Number of Ratings', 'Number of Reviews']])

pca.explained_variance_ratio_.round(3)

pca = PCA(n_components=1, random_state=123)

pca.fit(df[['Number of Ratings', 'Number of Reviews']])

df['num_feedback'] = pca.transform(df.loc[:, ('Number of Ratings', 'Number of Reviews')]).flatten()
df = df.drop(['Number of Ratings', 'Number of Reviews'], axis=1)

"""# **Data Splitting**"""

X = df.drop(['Price'], axis=1)
y = df['Price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)

print(f'X_train shape: {X_train.shape}')
print(f'X_test shape: {X_test.shape}')

"""# **Data Normalization**"""

scaler = StandardScaler()

num_cols = ['num_feedback']

X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols] = scaler.transform(X_test[num_cols])

X_train.head()

"""# **Modeling**"""

models = pd.DataFrame(index=['train_mae', 'test_mae'], 
                      columns=['rf', 'xgb'])

rfr = RandomForestRegressor()

rfr.fit(X_train, y_train)
 
models.loc['train_mae', 'rf'] = mean_absolute_error(y_pred=rfr.predict(X_train), y_true=y_train)

xgbr = xgb.XGBRegressor()

xgbr.fit(X_train, y_train)
 
models.loc['train_mae', 'xgb'] = mean_absolute_error(y_pred=xgbr.predict(X_train), y_true=y_train)

mae = pd.DataFrame(columns=['train', 'test'], index=['rf', 'xgb'])
 
model_dict = {'rf': rfr, 'xgb': xgbr}
 
for name, model in model_dict.items():
    mae.loc[name, 'train'] = mean_absolute_error(y_true=y_train, y_pred=model.predict(X_train))
    mae.loc[name, 'test'] = mean_absolute_error(y_true=y_test, y_pred=model.predict(X_test))

mae

fig, ax = plt.subplots()
mae.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

prediction = X_test.iloc[:10]

pred_dict = {'y_true': y_test[:10]}

for name, model in model_dict.items():
  pred_dict['prediction_' + name] = model.predict(prediction).round(2)
 
pd.DataFrame(pred_dict)

mae_target = (df['Price'].max() - df['Price'].min()) * 0.1
print(round(mae_target, 4))

"""# **Conclusion**

From the model evaluation, we can see that the MAE score of all the used models is all below 10%, with the XGBoost model that has the lowest MAE score. Therefore, it can be concluded that the model is a good fit.
"""